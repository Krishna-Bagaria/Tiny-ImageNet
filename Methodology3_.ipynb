{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import skimage\n",
    "from skimage.io import ImageCollection,concatenate_images,imread\n",
    "from skimage.transform import resize\n",
    "from skimage.color import gray2rgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.applications.resnet50 import decode_predictions\n",
    "#from keras.applications.xception import Xception\n",
    "from keras.applications import resnet50\n",
    "from keras.applications import xception\n",
    "from keras.applications import vgg16\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#import mahotas as mt\n",
    "#import xgboost as xg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#utility functions for pickling \n",
    "# dumping into pickle\n",
    "def pickleDump(dataToBeDumoped,name):\n",
    "    import pickle \n",
    "    dbfile = open(name, 'ab') \n",
    "    pickle.dump(dataToBeDumoped,dbfile)\n",
    "    dbfile.close()\n",
    "\n",
    "    \n",
    "#loadingPickle\n",
    "def loadPickleData(nameOfPickleFile):\n",
    "    import pickle \n",
    "    dbfile = open(nameOfPickleFile, 'rb')      \n",
    "    loaded = pickle.load(dbfile)\n",
    "    \n",
    "    return loaded\n",
    "#extract haralick from pickle file\n",
    "def extractHaralickPickle(path):\n",
    "    return loadPickleData(path+'haralick')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 576)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def returnImagesAndLabels(path):\n",
    "    \n",
    "    lab = pd.read_csv('dataset/train.csv')\n",
    "    images = []\n",
    "    files = []\n",
    "    labels = []\n",
    "    for index, row in lab.iterrows():\n",
    "        files.append(row['Id'])\n",
    "        labels.append(row['Category'])\n",
    "    target = []\n",
    "    #img_c = cv2.imread(path+ '/'+ files[i])\n",
    "    #img_c = cv2.cvtColor(img_c,cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    #cv2.normalize(histr,histr)\n",
    "    #fd = histr.flatten()\n",
    "    #hist.append(fd)\n",
    "    for i in range(len(files)):\n",
    "        name = path+'/'+files[i]\n",
    "        overallPath = path+'train/'+files[i]\n",
    "        img = cv2.imread(path+ 'train/'+ files[i],0)\n",
    "        #histr = cv2.calcHist([img],[0,1,2],None,[8,8,8],[0,256,0,256,0,256]) \n",
    "        images.append(img)\n",
    "        \n",
    "        target.append(labels[i])\n",
    "        \n",
    "    return images,target\n",
    "        \n",
    "#extracting images and their target/labels\n",
    "images, target = returnImagesAndLabels('dataset/')\n",
    "\n",
    "#converting them to numpy arrays\n",
    "images = np.array(images)\n",
    "target = np.array(target)\n",
    "\n",
    "\n",
    "def hogFeatures(pixels):\n",
    "    from skimage.feature import hog\n",
    "    hogs = []\n",
    "    for i in range(len(pixels)):\n",
    "        if(pixels[i].shape == 3):\n",
    "            fd = hog(pixels[i], orientations=16, pixels_per_cell=(16, 16),cells_per_block=(2, 2),multichannel=True)\n",
    "            hogs.append(fd)\n",
    "        else:\n",
    "            fd = hog(pixels[i], orientations=16, pixels_per_cell=(16, 16),cells_per_block=(2, 2))\n",
    "            hogs.append(fd)\n",
    "    return hogs\n",
    "\n",
    "\n",
    "#pass images as arguements\n",
    "hog = hogFeatures(images)\n",
    "hog = np.array(hog)\n",
    "print(hog.shape)\n",
    "\n",
    "\n",
    "\n",
    "haralick = extractHaralickPickle('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_haralick = np.concatenate((hog,haralick),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHuMoments(path):\n",
    "    return loadPickleData(path+'humom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "humom = getHuMoments('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_humom_haralick = np.concatenate((humom,hog,haralick),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef ensemble_result(X_train,y_train,X_test,y_test):\\n    \\n    preds = ensemble(X_train,y_train,X_test,y_test)\\n    final_preds = []\\n    for i in range(pred.shape[0]):\\n        mode = preds[i][j]\\n        for j in range(pred.shape[1]):\\n            \\n            \\n        \\n\\n        \\n        \\n    '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensemble(X_train,y_train,X_test,y_test):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "    clf1 = LogisticRegression( random_state=1)\n",
    "    clf2 = RandomForestClassifier(n_estimators=1000, random_state=1)\n",
    "    clf3 = GaussianNB()    \n",
    "    model = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],voting='soft')\n",
    "    model = model.fit(X_train, y_train)\n",
    "    print(model.score(X_test,y_test))\n",
    "        \n",
    "    \n",
    "    \n",
    "    '''avoid run NN and svm, doesnt affect the result much, but affect the run time'''\n",
    "    \n",
    "    '''\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    nn = MLPClassifier()\n",
    "    \n",
    "    nn.fit(X_train,y_train)\n",
    "    nn.predict(X_test,y_train)\n",
    "    \n",
    "    from sklearn.svm import SVC\n",
    "    svm = SVC(kernel = 'linear')\n",
    "    svm.fit(X_train,y_train)\n",
    "    svm.predict(X_test,y_train)\n",
    "    '''\n",
    "#    final_pred = stats.mode(predictions,axis=1)\n",
    "\n",
    "'''\n",
    "def ensemble_result(X_train,y_train,X_test,y_test):\n",
    "    \n",
    "    preds = ensemble(X_train,y_train,X_test,y_test)\n",
    "    final_preds = []\n",
    "    for i in range(pred.shape[0]):\n",
    "        mode = preds[i][j]\n",
    "        for j in range(pred.shape[1]):\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_humom_haralick = np.concatenate((humom,hog,haralick),axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(hog_humom_haralick, target, test_size=0.20, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 596), (8000,), (2000, 596), (2000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.249\n"
     ]
    }
   ],
   "source": [
    "ensemble(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
